
\begin{document}

\maketitle

\begin{abstract}
We start from an untyped, well-scoped Î»-calculus and introduce a bidirectional
typing relation corresponding to a Multiplicative-Additive Intuitionistic Linear
Logic. We depart from
typical presentations to adopt one that is well-suited to the intensional setting
of Martin-LÃ¶f Type Theory. This relation is based on the idea that a linear term
consumes some of the resources available in its context whilst leaving behind
leftovers which can then be fed to another program.

Concretely, this means that typing derivations have both an input
and an output context. This leads to a notion of weakening (the extra
resources added to the input context come out unchanged in the output
one), a rather direct proof of stability under substitution, an
analogue of the frame rule of separation logic showing that the
state of unused resources can be safely ignored, and a proof that
typechecking is decidable. Finally, we demonstrate that this alternative
formalization is sound and complete with respect to a more traditional
representation of Intuitionistic Linear Logic.

The work has been fully formalised in Agda, commented source files
are provided as additional material available at~\url{https://github.com/gallais/typing-with-leftovers}.
\end{abstract}

\section{Introduction}

The strongly-typed functional programming community has benefited from
a wealth of optimisations made possible precisely because the library
author as well as the compiler are aware of the type of the program they
are working on. These optimisations have ranged from Danvy's type-directed
partial evaluation~\cite{Danvy1999Type} residualising specialised programs
to erasure mechanisms --be they user-guided like Coq's extraction~\cite{letouzey2002new}
which systematically removes all the purely logical proofs put in Prop by
the developer or automated like Brady and Teji{\v{s}}c{\'a}k's erased
values~\cite{brady2003inductive,bradypractical}-- and including the library
defining the State-Thread~\cite{launchbury1994lazy} monad which relies on
higher-rank polymorphism and parametricity to ensure the safety of using an
actual mutable object in a lazy, purely functional setting.

However, in the context of the rising development of dependently-typed
programming languages~\cite{Brady2013idris, norell2009dependently} which,
unlike ghc's Haskell~\cite{weirich2013towards}, incorporate a hierarchy
of universes in order to ensure that the underlying logic is consistent,
some of these techniques are not applicable anymore. Indeed, the use of
large quantification in the definition of the ST-monad crucially relies
on impredicativity. As a consequence, the specification of programs
allowed to update a mutable object in a safe way has to change.
Idris has been extended with experimental support for uniqueness types
inspired by Clean's~\cite{achten1993high} and Rust's ownership types~\cite{manual:rust},
all of which stem from linear logic~\cite{girard1987linear}.

In order to be able to use type theory to formally study the meta-theory
of the programming languages whose type system includes notions of linearity,
we need to have a good representation of such constraints.

Section~\ref{sec:rawterms} introduces the well-scoped untyped Î»-calculus
we are going to use as our language of raw terms. Section~\ref{sec:typingrules}
defines the linear typing rules for this language as relations which
record the resources consumed by a program. The next sections are dedicated
to proving properties of this type system: Section~\ref{sec:framing}
proves that the status of unused variables rightfully does not matter,
Section~\ref{sec:weakening} (and respectively Section~\ref{sec:substitution})
demonstrates that these typing relations are stable under weakening
(respectively substitution), Section~\ref{sec:functional} demonstrates that
these relations are functional, and Section~\ref{sec:typechecking} that
they are decidable i.e. provides us with a typechecking algorithm. Finally
Section~\ref{sec:equivalence} goes back to a more traditional presentation
of Intuitionistic Multiplicative-Additive Linear Logic and demonstrates it is
equivalent to our type system.


\paragraph*{Notations} This whole development has been fully formalised
in Agda. Rather than using Agda's syntax, the results are reformulated
in terms of definitions, lemmas, theorems, and examples. However it is
important to keep in mind the distinction between various kinds of objects.
\texttt{Teletype} is used to denote data constructors, \DefinedType{small
capitals} are characteristic of defined types. A type family's index is
written as a subscript e.g. \Var{n}.

We use two kinds of inference rules to describe inductive families: double
rules are used to define types whilst simple ones correspond to constructors.
In each case the premises correspond to arguments (usually called parameters
and indices for types) and the conclusion shows the name of the constructor.
A typical example is the inductively defined set of unary natural numbers.
The inductive type is called \Nat{} and it has two constructors: $0$ takes
no argument whilst \natsucc{\cdot} takes a \Nat{} $n$ and represents its successor.
\begin{mathpar}
\type{ }{\Nat{} : \Set{}}
\and \constructor{ }{\texttt{\natzero} : \Nat{}}
\and \constructor{n : \Nat{}}{\natsucc{n} : \Nat{}}
\end{mathpar}

\section{The Calculus of Raw Terms}\label{sec:rawterms}

The calculus we study in this paper is meant to be a core language,
even though it will be rather easy to write programs in it. As a
consequence all the design choices have been guided by the goal
of facilitating its mechanical treatment in a dependently-typed language.
That is why we use de Bruijn indices to represent variable bindings.
We demonstrate in the code accompanying the paper how to combine a
parser and a scope checker to turn a surface level version of the
language using strings as variable names into this representation.

Following Bird and Patterson~\cite{bird_paterson_1999} and Altenkirch and Reus~\cite{altenkirch1999monadic},
we define the raw terms of our language not as an inductive
type but rather as an inductive \emph{family}~\cite{dybjer1994inductive}.
This technique, sometimes dubbed ``type-level de Bruijn indices'',
makes it possible to keep track, in the index of the family, of the
free variables currently in scope. As is nowadays folklore, instead of
using a set-indexed presentation where a closed terms is indexed by
the empty set $âŠ¥$ and fresh variables are introduced by wrapping
the index in a \texttt{Maybe} type constructor\footnote{The value
\texttt{nothing} represents the fresh variable whilst the constructor
\texttt{just} lifts the other ones in the new scope.}, we index
our terms by a natural number instead. The
\Var{} type family\footnote{It is also known as \texttt{Fin} (for
``finite set'') in the dependently typed programming community.}
defined below represents the de Bruijn indices~\cite{debruijn1972lambda}
corresponding to the $n$ free variables present in a scope $n$.

\begin{mathpar}
\type{n : \Nat{}}{\Var{n} : \Set{}}
\and \constructor{ }{\varzero{} : \Var{\natsucc{n}}}
\and \constructor{k : \Var{n}}{\varsucc{k} : \Var{\natsucc{n}}}
\end{mathpar}

We present the calculus in a bidirectional fashion~\cite{pierce2000local}.
This definition style scales well to more complex type theories where full
type-inference is not tractable anymore whilst keeping the type annotations
the programmer needs to add to a minimum. The term constructors of the calculus
are split in two different syntactic categories corresponding to constructors
of canonical values on one hand and eliminators on the other. These categories
characterise the flow of information during typechecking: given a context
assigning a type to each free variable, canonical values (which we call \Checkable{})
can be \emph{check}ed against a type whilst we may \emph{infer} the type of
computations (which we call \Inferable{}). Each type is indexed by a scope:

\begin{mathpar}
\type{n : \Nat{}}{\Inferable{n} : \Set{}}
\and \type{n : \Nat{}}{\Checkable{n} : \Set{}}
\end{mathpar}

On top of the constructors one would expect for a usual definition of
the untyped Î»-calculus (\var{\cdot}, \app{\cdot}{\cdot}, and \lam{\cdot})
we have constructors and eliminators for sums (\inl{\cdot}, \inr{\cdot},
\cas{\cdot}{\cdot}{\cdot}{\cdot}), products (\prd{\cdot}, \letin{\cdot}{\cdot}{\cdot},
\prl{\cdot}, \prr{\cdot}), unit (\uni{}, \letin{\cdot}{\cdot}{\cdot})
and void (\exf{\cdot}{\cdot}). Two additional rules (\neu{\cdot} and
\cut{\cdot}{\cdot} respectively) allow the embedding of \Inferable{}
into \Checkable{} and vice-versa. They make it possible to form redexes
by embedding canonical values into computations and then applying
eliminators to them. In terms of typechecking, they correspond to a
change of direction between inferring and checking.

\begin{figure}[H]\centering
\begin{tabular}{lcl}
âŸ¨\Inferable{n}âŸ© & ::= & \var{âŸ¨\Var{n}âŸ©} \\
                   &  |  & \app{âŸ¨\Inferable{n}âŸ©}{âŸ¨\Checkable{n}âŸ©} \\
                   &  |  & \cas{âŸ¨\Inferable{n}âŸ©}{âŸ¨\Type{}âŸ©}{âŸ¨\Checkable{\natsucc{n}}âŸ©}{âŸ¨\Checkable{\natsucc{n}}âŸ©} \\
                   &  |  & \prl{âŸ¨\Inferable{n}âŸ©} ~|~ \prr{âŸ¨\Inferable{n}âŸ©} \\
                   &  |  & \exf{âŸ¨\Type{}âŸ©}{âŸ¨\Inferable{n}âŸ©} \\
                   &  |  & \cut{âŸ¨\Checkable{n}âŸ©}{âŸ¨\Type{}âŸ©} \\ \\

âŸ¨\Checkable{n}âŸ© & ::= & \lam{âŸ¨\Checkable{\natsucc{n}}âŸ©} \\
                   &  |  & \letin{âŸ¨\Pattern{m}âŸ©}{âŸ¨\Inferable{n}âŸ©}{âŸ¨\Checkable{m + n}âŸ©} \\
                   &  |  & \uni{} \\
                   &  |  & \inl{âŸ¨\Checkable{n}âŸ©} ~|~ \inr{âŸ¨\Checkable{n}âŸ©} \\
                   &  |  & \prd{âŸ¨\Checkable{n}âŸ©}{âŸ¨\Checkable{n}âŸ©} \\
                   &  |  & \neu{âŸ¨\Inferable{n}âŸ©} \\
\end{tabular}
\caption{Grammar of the Language of Raw Terms}
\end{figure}

The constructors \texttt{cut}, \texttt{case}, and \texttt{exfalso}
take an extra \Type{} argument in order to guarantee the success
and uniqueness of type-inference for \Inferable{} terms.

A notable specificity of this language is the ability to use nested
patterns in a let binder rather than having to resort to cascading
lets. This is achieved thanks to a rather simple piece of kit: the
\Pattern{} type family. A value of type $\Pattern{n}$ represents an
irrefutable pattern binding $n$ variables. Because variables are
represented as de Bruijn indices, the base pattern does not need to
be associated with a name, it simply is a constructor \texttt{v}
binding exactly one variable. The brackets pattern \texttt{âŸ¨âŸ©} matches
unit values and binds nothing. The comma pattern constructor takes
two nested patterns respectively binding $m$ and $n$ variables and
uses them to deeply match a pair thus binding $(m + n)$ variables.

\begin{mathpar}
\type{n : \Nat{}}{\Pattern{n} : \Set{}}
\and \constructor{ }{\texttt{v} : \Pattern{1}}
\and \constructor{ }{\texttt{âŸ¨\!âŸ©} : \Pattern{0}}
\and \constructor{p : \Pattern{m} \and q : \Pattern{n}}{p \texttt{,} q : \Pattern{m + n}}
\end{mathpar}

The grammar of raw terms only guarantees that all expressions are
well-scoped by construction. It does not impose any other constraint,
which means that a user may write valid programs but also invalid
ones as the following examples demonstrate:

\begin{example}\label{example:swap}
\texttt{swap} is a closed, well-typed linear term taking a pair as
an input and swapping its components. It corresponds to the mathematical
function $(x, y) \mapsto (y, x)$.
\begin{lstlisting}
  swap = lam (let (v , v) := var z
              in prd (neu (var (s z))) (neu (var z)))
\end{lstlisting}
\end{example}

\begin{example}\label{example:illTyped}
\texttt{illTyped} is a closed linear term. However it is manifestly
ill-typed: the let-binding it uses tries to break down a function as
if it were a pair.
\begin{lstlisting}
  illTyped = let (v , v) := cut (lam (neu (var z))) (a âŠ¸ a)
             in prd (neu (var z)) (neu (var (s z)))
\end{lstlisting}
\end{example}

\begin{example}\label{example:diagonal}
Finally, \texttt{diagonal} is a term typable in the simply-typed
lambda calculus but it is not linear: it duplicates its input just
like $x \mapsto (x, x)$ does.
\begin{lstlisting}
  diagonal = lam (prd (neu (var z)) (neu (var z)))
\end{lstlisting}
\end{example}

\section{Linear Typing Rules}\label{sec:typingrules}

These examples demonstrate that we need to define a typing
relation describing the rules terms need to abide by in order
to qualify as well-typed linear programs. We start by defining
the types our programs may have using the grammar in Figure~\ref{fig:types}.
Apart from the usual linear type formers, we have a constructor
\Base{} which makes it possible to have countably many different
base types.

\begin{figure}[ht]\centering
\begin{tabular}{lcl}
âŸ¨\Type{}âŸ© & ::= & \Base{âŸ¨$\mathbb{N}$âŸ©}
             ~|~    \Zero{}
             ~|~    \Unit{} \\
            & |   & \Lolli{âŸ¨\Type{}âŸ©}{âŸ¨\Type{}âŸ©}
             ~|~    \Tensor{âŸ¨\Type{}âŸ©}{âŸ¨\Type{}âŸ©} \\
            & |   & \Sum{âŸ¨\Type{}âŸ©}{âŸ¨\Type{}âŸ©}
             ~|~    \With{âŸ¨\Type{}âŸ©}{âŸ¨\Type{}âŸ©}
\end{tabular}
\caption{Grammar of \Type{}}\label{fig:types}
\end{figure}

A linear type system is characterised by the fact that all the resources
available in the context have to be used exactly once by the term being
checked. In traditional presentations of linear logic this is achieved by
representing the context as a multiset and, in each rule, cutting it up
and distributing its parts among the premises. This is epitomised by the
introduction rule for tensor.

However, multisets are an intrinsically \emph{extensional} notion and
therefore quite arduous to work with in an \emph{intensional} type theory.
Various strategies can be applied to tackle this issue; most of them rely
on using linked lists to represent contexts together with ways to reorganise
the context.

In Figure~\ref{rule:tensor} we show two of the most common representations
of the tensor rule. The first one splits the context into Î“ and Î” and dispatches
them into the subproofs; it relies on the existence of structural rules which
the user will be able to use to reorganise the context appropriately. The second
one is a combined rule letting the user re-arrange the context on the fly by using
the notion of ``bag-equivalence'' for lists denoted $\_â‰ˆ\_$.

\begin{figure}[ht]
\begin{mathpar}
\inferrule
 {Î“ âŠ¢ Ïƒ \and Î” âŠ¢ Ï„
}{Î“, Î” âŠ¢ Ïƒ âŠ— Ï„
}{âŠ—_i}

\and \inferrule
 {Î“ âŠ¢ Ïƒ \and Î” âŠ¢ Ï„ \and Î“, Î” â‰ˆ Î˜
}{Î˜ âŠ¢ Ïƒ âŠ— Ï„
}{âŠ—_i}
\end{mathpar}
\caption{Introduction rules for tensor (left: usual presentation, right: with reordering on the fly)\label{rule:tensor}}
\end{figure}

In both of these cases, the user has to explicitly rearrange the context
either by using structural rules or proving that two distinct contexts are
bag equivalent. Although one can find coping mechanisms to handle such
clunky systems (for instance using a solver for bag-equivalence~\cite{danielsson2012bag}
based on the proof-by-reflection~\cite{boutin1997using} approach to
automation), we would rather not.

All of these strategies are artefacts of the unfortunate mismatch
between the ideal mathematical objects one wishes to model and
their internal representation in the proof assistant. Short of
having proper quotient types, this will continue to be an issue
when dealing with multisets. The solution described in the rest
of this paper is syntax-directed; it does not try to replicate a
set-theoretic approach in intuitionistic type theory but rather
strives to find the type theoretical structures which can make
the problem more tractable. Indeed, given the right abstractions
most proofs become direct structural inductions.

\subsection{Usage Annotations}

McBride's recent work~\cite{mcbride2016got} on combining linear and
dependent types highlights the distinction one can make between
referring to a resource and actually consuming it. In the same spirit,
rather than dispatching the available resources in the appropriate
subderivations, we consider that a term is checked in a \emph{given}
context on top of which usage annotations are super-imposed. These
usage annotations indicate whether resources have been consumed already
or are still available. Type-inference (resp. Type-checking) is then
inferring (resp. checking) a term's type but \emph{also} annotating
the resources consumed by the term in question and returning the
\emph{leftovers} which gave their name to this paper.

\begin{definition}
\label{definition:context}
A \textbf{\Context{}} is a list of \Type{}s indexed by its length. It can
be formally described by the following inference rules:
\begin{mathpar}
\type{n : \Nat{}}{\Context{n} : \Set{}}
\and \constructor{ }{[] : \Context{0}}
\and \constructor{Î³ : \Context{n} \and Ïƒ : \Type{}}{Î³ âˆ™ Ïƒ : \Context{\natsucc{n}}}
\end{mathpar}
\end{definition}


\begin{definition}
\label{definition:usage}
A \textbf{\Usage{}} is a predicate on a type $Ïƒ$ describing whether the
resource associated to it is available or not. We name the
constructors describing these two states \fresh{} (for \textbf{fresh})
and \stale{} (for \textbf{stale}) respectively.
These are naturally lifted to contexts in a pointwise manner and we reuse
the \Usages{} name and the \fresh{} and \stale{} names for the functions
taking a context and returning either a fully fresh or fully stale \Usages{}
for it.

\begin{mathpar}
\type
 {Ïƒ : \Type{}
}{\Usage{Ïƒ} : \Set{}
}
\and\constructor
 {
}{\fresh{Ïƒ} : \Usage{Ïƒ}
}
\and\constructor
 {
}{\stale{Ïƒ} : \Usage{Ïƒ}
}
\end{mathpar}
\begin{mathpar}
\type
 { Î³ : \Context{n}
}{\Usages{Î³} : \Set{}
}
\and\constructor
 {
}{[] : \Usages{[]}
}
\and\constructor
 {Î“ : \Usages{Î³} \and S : \Usage{Ïƒ}
}{Î“ âˆ™ S : \Usages{Î³ âˆ™ Ïƒ}
}
\end{mathpar}
\end{definition}

\subsection{Typing as Consumption Annotation}

A Typing relation seen as a consumption annotation process describes what it
means to analyze a term in a scope of size $n$: given a context of types for
these $n$ variables, and a usage annotation for that context, it ascribes a
type to the term whilst crafting another usage annotation containing all the
leftover resources. Formally:

\begin{definition}
\label{definition:typing}
A \textbf{Typing Relation} for $T$ a \Nat{}-indexed inductive family is
an indexed relation \TR{n} such that:
\begin{mathpar}
\type
 {n : \Nat{} \and Î³ : \Context{n} \and Î“, Î” : \Usages{Î³} \and t : T_n \and Ïƒ : \Type{}
}{\text{\TR{n}}(Î“, t, Ïƒ, Î”) : \Set{}
}
\end{mathpar}
\end{definition}

This definition clarifies the notion but also leads to more generic
statements later on: weakening, substitution, framing can all be
expressed as properties a Typing Relation might have. We can already
list the typing relations introduced later on in this article which
fit this pattern. We have split their arguments into three columns
depending on whether they should be understood as either inputs (the
known things), scrutinees (the things being validated), or outputs
(the things that we learn) and hint at what the flow of information
in the typechecker will be.

\input{typing-rules}

\begin{remark}The use of \andalso{} is meant to suggest that the input Î“
gets distributed between the type Ïƒ of the term and the leftovers
Î” obtained as an output. Informally $Î“ \simeq Ïƒ âŠ— Î”$, hence the
use of a tensor-like symbol.
\end{remark}

\subsubsection{Typing de Bruijn indices}

The simplest instance of a Typing Relation is the one for de Bruijn
indices: given an index $k$ and a usage annotation, it successfully
associates a type $Ïƒ$ to that index if and only if the $k$th resource
in context is of type $Ïƒ$ and fresh (i.e. its \Usage{Ïƒ} is \fresh{Ïƒ}).
In the resulting leftovers, this resource will have turned stale (\stale{Ïƒ})
because it has now been used:

\begin{definition}
The typing relation for \Var{} is presented in a sequent-style: Î“ âŠ¢$_v$ $k$ âˆˆ Ïƒ \andalso{} Î”
means that starting from the usage annotation Î“, the de Bruijn index
$k$ is ascribed type Ïƒ with leftovers Î”. It is defined inductively by
two constructors (cf. Figure~\ref{figure:deBruijn}).
\end{definition}

\input{typing-rules-var}

\begin{remark}The careful reader will have noticed that there is precisely
one typing rule for each \Var{} constructor. It is not a coincidence. And
if these typing rules are not named it's because in Agda, they can
be given the same name as their \Var{} counterpart and the typechecker will
perform type-directed disambiguation. The same will be true for \Inferable{},
\Checkable{} and \Pattern{} which means that writing down a typable program
could be seen as either writing a raw term or the typing derivation associated
to it depending on the author's intent.
\end{remark}

\begin{example}\label{example:debruijn}
The de Bruijn index 1 has type Ï„ in the context (Î³ âˆ™ Ï„ âˆ™ Ïƒ) with
usage annotation (Î“ âˆ™ \fresh{Ï„} âˆ™ \fresh{Ïƒ}), no matter what Î“
actually is:
\begin{mathpar}
\inferrule
 {\inferrule
   {
  }{Î“ âˆ™ \fresh{Ï„} âŠ¢ \varzero{} âˆˆ Ï„ \andalso{} Î“ âˆ™ \stale{Ï„}
  }
}{Î“ âˆ™ \fresh{Ï„} âˆ™ \fresh{Ïƒ} âŠ¢ \varsucc{\varzero} âˆˆ Ï„ \andalso{} Î“ âˆ™ \stale{Ï„} âˆ™ \fresh{Ïƒ}
}
\end{mathpar}
Or, as it would be written in Agda, taking advantage of the fact that
the language constructs and the typing rules about them have been given
the same names:
\begin{lstlisting}
  one : 'Î“' 'âˆ™' f 'Ï„' 'âˆ™' f 'Ïƒ' âŠ¢ s z 'âˆˆ' 'Ï„' '\andalso{}' 'Î“' 'âˆ™' s 'Ï„' 'âˆ™' f 'Ïƒ'
  one = s z
\end{lstlisting}
\end{example}

\subsubsection{Typing Terms}

We now face compound untyped terms such as \app{f}{t} whose subterms $f$ and $t$
have been defined in the \emph{same} scope of size $n$. Therefore the typing relation
for these terms needs to use the \emph{same} context of size $n$ for both premises.
Trying to cut up a \Context{n} in two just like in Figure~\ref{rule:tensor} would
not only be cumbersome, it wouldn't be type correct. This is where usage annotations
shine.

The key idea appearing in all the typing rules for compound
expressions is to use the input \Usages{} to type one of the
sub-expressions, collect the leftovers from that typing
derivation and use them as the new input \Usages{} when typing
the next sub-expression.

Another common pattern can be seen across all the rules involving
binders, be they Î»-abstractions, let-bindings or branches of a
case. Typechecking the body of a binder involves extending the
input \Usages{} with fresh variables and observing that they have
become stale in the output one. This guarantees that these bound
variables cannot escape their scope as well as that they have indeed
been used. Although not the focus of this paper, it is worth noting
that relaxing the staleness restriction would lead to an affine
type system which would be interesting in its own right.

\begin{definition}The Typing Relation for \Inferable{} is typeset
in a fashion similar to the one for \Var{}: in both cases
the type is inferred. $Î“ âŠ¢ t âˆˆ Ïƒ \andalso{} Î”$ means that given Î“ a
$\Usages{Î³}$, and $t$ an \Inferable{}, the type Ïƒ is inferred
together with leftovers Î”, another $\Usages{Î³}$. The rules are
listed in Figure~\ref{figure:infer}.
\end{definition}

\input{typing-rules-infer}

\begin{definition}For \Checkable{}, the type Ïƒ comes first: Î“ âŠ¢ Ïƒ âˆ‹ t \andalso{} Î” means
that given Î“ a \Usages{Î³}, a type Ïƒ, the \Checkable{} $t$ can
be checked to have type Ïƒ with leftovers Î”. The rules can be found
in Figure~\ref{figure:check}.
\end{definition}

\input{typing-rules-check}

We can see that both variants of a product type --tensor (âŠ—) and
with (\&)-- use the same surface language constructor but are
disambiguated in a type-directed manner in the checking relation.
The premises are naturally widely different: With lets its user
pick which of the two available types they want and as a consequence
both components have to be proven using the same resources. Tensor
on the other hand forces the user to use both so the leftovers
are threaded from one premise to the other.

\begin{definition}
Finally, \Pattern{}s are checked against a type and a context of
newly bound variables is generated. If the variable pattern always
succeeds, the pair constructor pattern on the other hand
only succeeds if the type it attempts to split is a tensor type.
The context of newly-bound variables is then the collection of the
contexts associated to the nested patterns. The rules are given in
Figure~\ref{figure:pattern}.
\end{definition}

\input{typing-rules-pattern}

\begin{example}
Given these rules, we see that the identity function can be checked
at type (Ïƒ âŠ¸ Ïƒ) in an empty context:
\begin{mathpar}
\inferrule
 {\inferrule
   {\inferrule
     {\inferrule
       {
      }{[] âˆ™ \fresh{Ïƒ} âŠ¢_v \varzero âˆˆ Ïƒ \andalso{} [] âˆ™ \stale{Ïƒ}
      }
    }{[] âˆ™ \fresh{Ïƒ} âŠ¢ \var{\varzero} âˆˆ Ïƒ \andalso{} [] âˆ™ \stale{Ïƒ}
    }
  }{[] âˆ™ \fresh{Ïƒ} âŠ¢ Ïƒ âˆ‹ \neu{(\var{\varzero})} \andalso{} [] âˆ™ \stale{Ïƒ}
  }
}{[] âŠ¢ Ïƒ âŠ¸ Ïƒ âˆ‹ \lam{(\neu{(\var{\varzero})})} \andalso{} []
}
\end{mathpar}
Or, as it would be written in Agda where the typing rules were
given the same name as their term constructor counterparts:
\begin{lstlisting}
  identity : [] 'âŠ¢' 'Ïƒ' 'âŠ¸' 'Ïƒ' 'âˆ‹' lam (neu (var z)) '\andalso{}' []
  identity = lam (neu (var z))
\end{lstlisting}
\end{example}

\begin{example}\label{example:swapTyped}
It is also possible to revisit Example \ref{example:swap} to prove
that \texttt{swap} can be checked against type (Ïƒ âŠ— Ï„) âŠ¸ (Ï„ âŠ— Ïƒ) in an empty
context. This gives the lengthy derivation included in the appendix
or the following one in Agda which is quite a lot more readable:

\begin{lstlisting}
  swapTyped : [] 'âŠ¢' ('Ïƒ' 'âŠ—' 'Ï„') 'âŠ¸' ('Ï„' 'âŠ—' 'Ïƒ') 'âˆ‹' swap '\andalso{}' []
  swapTyped = lam (let (v , v) := var z
                   in prd (neu (var (s z))) (neu (var z))
\end{lstlisting}
\end{example}

%%%%%%%%%%%%%
%% FRAMING %%
%%%%%%%%%%%%%

\section{Framing}\label{sec:framing}

The most basic property one can prove about this typing system is
the fact that the state of the resources which are not used by a
lambda term is irrelevant. We call this property the Framing
Property because of the obvious analogy with the frame rule in
separation logic. This can be reformulated as the fact that as
long as two pairs of an input and an output \Usages{} exhibit the
same consumption pattern then if a derivation uses one of these,
it can use the other one instead. Formally (postponing the
definition of $Î“ - Î” â‰¡ Î˜ - Î$):

\begin{definition}A Typing Relation \TR{\cdot{}} for a \Nat{}-indexed
family $T$ has the \textbf{Framing Property} if for all $k$ a \Nat{},
Î³ a \Context{k}, Î“, Î”, Î˜, Î four \Usages{Î³}, $t$ an element
of $T_k$ and Ïƒ a Type, if \mbox{Î“ â”€ Î” â‰¡ Î˜ â”€ Î} and \TR{k}(Î“, t, Ïƒ, Î”)
then \TR{k}(Î˜, t, Ïƒ, Î) also holds.
\end{definition}

\begin{remark}This is purely a property of the type system as
witnessed by the fact that the term $t$ is left unchanged wich
won't be the case when defining stability under Weakening or
Substitution for instance.
\end{remark}


\begin{definition}
\label{definition:differences}
\textbf{Consumption Equivalence} for a given \Context{} Î³ characterises
the pairs of an input and an output \Usages{Î³} which have the same consumption
pattern. The usages annotations for the empty context are trivially related.
If the context is not empty, then there are two cases: if the
resource is left untouched on one side, then so should it on the other
side but the two annotations may be different (here denoted $A$ and $B$
respectively). On the other hand, if the resource has been consumed
on one side then it has to be on the other side too.
\begin{mathpar}
\type
 {Î“, Î”, Î˜, Î : \Usages{Î³}
}{Î“ â”€ Î” â‰¡ Î˜ â”€ Î : \Set{}
}
\and \constructor
 {
}{[] â”€ [] â‰¡ [] â”€ []
}{
}
\\ \constructor
 {Î“ â”€ Î” â‰¡ Î˜ â”€ Î
}{(Î“ âˆ™ A) â”€ (Î” âˆ™ A) â‰¡ (Î˜ âˆ™ B) â”€ (Î âˆ™ B)
}{
}
\and \constructor
 {Î“ â”€ Î” â‰¡ Î˜ â”€ Î
}{(Î“ âˆ™ \fresh{Ïƒ}) â”€ (Î” âˆ™ \stale{Ïƒ}) â‰¡ (Î˜ âˆ™ \fresh{Ïƒ}) â”€ (Î âˆ™ \stale{Ïƒ})
}{
}
\end{mathpar}
\end{definition}

\begin{remark}Two pairs of usages which are consumption equivalent are defined
over the \emph{same} context (and thus scope). Stability of a typing rule with
respect to consumption equivalence will not be sufficient to introduce new
variables. This will be dealth with by defining weakening in Section~\ref{sec:weakening}.
\end{remark}

\begin{definition}The \textbf{Consumption Partial Order} Î“ âŠ† Î” is defined as
\mbox{Î“ â”€ Î” â‰¡ Î“ â”€ Î”}. It orders \Usages{} from least consumed to maximally consumed.
\end{definition}

\begin{lemma} The following properties on the Consumption relations hold:
\begin{enumerate}
  \item The consumption equivalence is a partial equivalence~\cite{mitchell1996foundations}.
  \item The consumption partial order is a partial order.
  \item If there is a \Usages{} Î§ ``in between'' two others Î“ and Î” according to
        the consumption partial order (i.e. $Î“ âŠ† Î§$ and $Î§ âŠ† Î”$), then any pair
        of \Usages{} Î˜, Î consumption equal to Î“ and Î” (i.e. \mbox{$Î“ â”€ Î” â‰¡ Î˜ â”€ Î$})
        can be split in a manner compatible with Î§. In other words: one can find
        Î– such that \mbox{$Î“ â”€ Î§ â‰¡ Î˜ â”€ Î–$} and \mbox{$Î§ â”€ Î” â‰¡ Î– â”€ Î$}.
\end{enumerate}
\end{lemma}

\begin{lemma}[Consumption]The Typing Relations for \Var{}, \Inferable{}
and \Checkable{} all imply that if a typing derivation exists with input
\Usages{} annotation Î“ and output \Usages{} annotation Î” then $Î“ âŠ† Î”$.\label{lemma:consumption}
\end{lemma}

\begin{theorem}
\label{theorem:framing}
The Typing Relation for \Var{} has the Framing Property.
So do the ones for \Inferable{} and \Checkable{}.
\end{theorem}
\begin{proof}
The proofs are by structural induction on the typing derivations. They rely on
the previous lemmas to, when faced with a rule with multiple premises and leftover
threading, generate the inclusion evidence and use it to split up the witness
of consumption equivalence and distribute it appropriately in the induction hypotheses.
\end{proof}


\begin{example}Coming back to the typing derivation for the de Bruijn index $1$ in
Example~\ref{example:debruijn}, we can use the Framing theorem to transport the proof
that \mbox{Î“ âˆ™ \fresh{Ï„} âˆ™ \fresh{Ïƒ} âŠ¢ \varsucc{\varzero} âˆˆ Ï„ \andalso{} Î“ âˆ™ \stale{Ï„} âˆ™ \fresh{Ïƒ}}
to a proof that \mbox{Î” âˆ™ \fresh{Ï„} âˆ™ \stale{Ïƒ} âŠ¢ \varsucc{\varzero} âˆˆ Ï„ \andalso{} Î” âˆ™ \stale{Ï„} âˆ™ \stale{Ïƒ}}
for any Î”. Indeed, we can see that these two pairs of \Usages{} are consumption equivalent
(\mbox{Î“ â”€ Î“ â‰¡ Î” â”€ Î”} holds by induction):
\begin{mathpar}
\inferrule
 {\inferrule
   {\inferrule
     {\vdots
     }{Î“ â”€ Î“ â‰¡ Î” â”€ Î”}
  }{Î“ âˆ™ \fresh{Ï„} â”€ Î“ âˆ™ \stale{Ï„} â‰¡ Î” âˆ™ \fresh{Ï„} â”€ Î” âˆ™ \stale{Ï„}
  }
}{Î“ âˆ™ \fresh{Ï„} âˆ™ \fresh{Ïƒ} â”€ Î“ âˆ™ \stale{Ï„} âˆ™ \fresh{Ïƒ} â‰¡ Î” âˆ™ \fresh{Ï„} âˆ™ \stale{Ïƒ} â”€ Î” âˆ™ \stale{Ï„} âˆ™ \stale{Ïƒ}
 }
\end{mathpar}
\end{example}

%%%%%%%%%%%%%%%
%% WEAKENING %%
%%%%%%%%%%%%%%%

\section{Weakening}\label{sec:weakening}

It is perhaps surprising to find a notion of weakening for a linear
calculus: the whole point of linearity is precisely to ensure that
all the resources are used. However, when opting for a system based
on consumption annotations it becomes necessary to be able to extend
the context a term lives in. This will typically be used in the definition
of parallel substitution to push the substitution under a binder. Linearity
is guaranteed by ensuring that the inserted variables are left untouched by
the term.

Weakening arises from a notion of inclusion. The appropriate type
theoretical structure to describe these inclusions is well-known
and called an Order Preserving Embeddding~\cite{chapman2009thesis,altenkirch1995categorical}.
Unlike a simple function witnessing the inclusion of its domain
into its codomain, the restriction brought by order preserving
embeddings guarantees that contraction is simply not possible which
is crucial in a linear setting.

\begin{definition}
An \textbf{Order Preserving Embedding} (OPE) is an inductive family. Its
constructors (dubbed ``moves'' in this paper) describe a strategy to realise
the promise of an injective embedding which respects the order induced by
the de Bruijn indices. We start with an example in Figure~\ref{figure:exampleOpe}
before giving, in Figure~\ref{figure:ope}, the formal definition of OPEs for
\Nat{}, \Context{} and \Usages{}.
\end{definition}

In the following example, we prove that the source context $\gamma âˆ™ \tau$
can be safely embedded into the target one $\gamma âˆ™ \sigma âˆ™ \tau âˆ™ \nu$,
written $\gamma âˆ™ \tau \leq \gamma âˆ™ \sigma âˆ™ \tau âˆ™ \nu$. This example proof
uses all three of the moves the inductive definition of OPEs offers:
\opeinsert{\alpha} which introduces a new variable of type $\alpha$,
\opecopy{} which embeds the source context's top variable, and \opedone{}
which simply copies the source context. Because we read
strategies left-to-right and it is easier to see how they act if contexts
are also presented left-to-right, we temporarily switch to \emph{cons}-style
(i.e. $\sigma , \gamma$) instead of the \emph{snoc}-style (i.e. $\gamma âˆ™ \sigma$)
used in the rest of this paper.

\begin{example} An Order Preserving Embedding proving:
$\gamma âˆ™ \tau \leq \gamma âˆ™ \sigma âˆ™ \tau âˆ™ \nu$

\begin{figure}[ht]\centering
\begin{tabular}{l|ccccccc}
\textit{OPE} & $\texttt{insert}_{\nu}$ &
             & \texttt{copy} &
             & $\texttt{insert}_{\sigma}$ &
             & \texttt{done}\\
\textit{source} & &
                & $\tau$ & ,
                & &
                & $\gamma$ \\
\textit{target} & $\nu$ & ,
                & $\tau$ & ,
                & $\sigma$ & ,
                & $\gamma$ \\
\end{tabular}
\label{figure:exampleOpe}
\end{figure}
\end{example}

Now that we have seen an example, we can focus on the formal definition.
We give the definition of \OPE{} for \Nat{}, \Context{} and \Usages{} all
side by side in one table: the first column lists the names of the
constructors associated to each move whilst the other ones give their
corresponding types for each category. It is worth noting that \OPE{}s for
\Context{} are indexed over the ones for \Nat{} and the \OPE{}s for \Usages{}
are indexed by both. The latter definitions are effectively algebraic
\emph{ornaments}~\cite{dagand2014transporting, mcbride2010ornamental} over
the previous ones, that is to say they have the same structure only storing
additional information.

\begin{figure}[H]\centering
\begin{tabular}{l|c|c|c}
& \Nat{} & \Context{} & \Usages{} \\
\texttt{done}
& \constructor
 {
}{k â‰¤ k
}
& \constructor
 {
}{\gamma â‰¤ \gamma
}
& \constructor
 {
}{\Gamma â‰¤ \Gamma
}\\ & & \\
\texttt{copy}
& \constructor
 {k â‰¤ l
}{\natsucc{k} â‰¤ \natsucc{l}
}
& \constructor
 {\gamma â‰¤ \delta
}{\gamma âˆ™ \sigma â‰¤ \delta âˆ™ \sigma
}
& \constructor
 {\Gamma â‰¤ \Delta  \and S : \Usages{\sigma}
}{\Gamma âˆ™ S â‰¤ \Delta âˆ™ S
}\\ & & \\
\texttt{insert}
& \constructor
 {k â‰¤ l
}{k â‰¤ \natsucc{l}
}
& \constructor
 {\gamma â‰¤ \delta
}{\gamma â‰¤ \delta âˆ™ \sigma
} & \constructor
  {\Gamma â‰¤ \Delta \and S : \Usages{\sigma}
}{\Gamma â‰¤ \Delta âˆ™ S
}
\end{tabular}
\caption{Order Preserving Embeddings for \Nat{}, \Context{} and $\Usages{}$\label{figure:ope}}
\end{figure}

\begin{itemize}

\item The first row defines the move \opedone{}. It is the strategy
corresponding to the trivial embedding of a set into itself by
the identity function and serves as a base case.

\item The second row corresponds to the \texttt{copy} move which extends
an existing embedding by copying the current $0$th variable from
source to target. The corresponding cases for \Context{}s and
$\Usages{}$ are purely structural: no additional content is required
to be able to perform a \texttt{copy} move.

\item The third row describes the move \texttt{insert}
which introduces an extra variable in the target set. This is the
move used to extend an existing context, i.e. to weaken it. In this
case, it is paramount that the OPE for \Context{}s should take a
type Ïƒ as an extra argument (it will be the type of the newly introduced
variable) whilst the OPE for $\Usages{}$ takes a $\Usage{Ïƒ}$ (it will
be the usage associated to that newly introduced variable of type Ïƒ).
\end{itemize}

Now that the structure of these OPEs is clear, we have to introduce a caveat
regarding this description: the \Context{} and \Usages{} case are a bit special.
They do not in fact mention the source and target sets in their indices. This
is a feature: when weakening a typing relation, the OPE for \Usages{} will be
applied simultaneously to the input \emph{and} the output \Usages{} which,
although of a similar structure because of their shared \Context{} index,
will be different.

\begin{definition}The \textbf{semantics of an OPE} is defined by induction
over the proof object. We use the overloaded function name \ope{\cdot} for it.
They behave as the simplified view given in Figure~\ref{figure:ope} where
$\gamma$ / $\Gamma$ is seen as the input, $\sigma$ / $S$ the additional
information stored into the proof object and $\delta$ / $\Delta$ the output.
\end{definition}

We leave out the definition of weakening for raw terms which is the standard
definition for the untyped Î»-calculus. It proves that given $k \leq l$ we can
turn an \Inferable{k} (respectively \Checkable{k}) into an \Inferable{l}
(respectively \Checkable{l}). It is given by a simple structural induction on
the terms themselves, using \opecopy{} to go under binders.

\begin{definition}A Typing Relation \TR{\cdot} for a \Nat{}-indexed family $T$
such that we have a function $\texttt{weak}_T$ transporting proofs that
$k â‰¤ l$ to functions $T_k â†’ T_l$ is said to have the \textbf{Weakening Property}
if for all $k, l$ in \Nat{}, $o$ a proof that $k â‰¤ l$, $O$ a proof that
$\OPE{}(o)$ and $ğ“$ a proof that $\OPE{}(O)$ then for all Î³ a $\Context{k}$,
Î“ and Î” two $\Usages{Î³}$, $t$ an element of $T_k$ and Ïƒ a \Type{}, if
\TR{k}$(Î“, t, Ïƒ, Î”)$ holds true then we also have
\TR{l}$(\ope{ğ“, Î“}, \texttt{weak}_T(o, t), Ïƒ, \ope{ğ“, Î”})$.
\end{definition}

\begin{theorem}The Typing Relation for \Var{} has the Weakening Property.
So do the Typing Relations for \Inferable{} and \Checkable{}.
\end{theorem}
\begin{proof}
The proof for \Var{} is by induction on the typing derivation. The
statements for \Inferable{} and \Checkable{} are proved by mutual
structural inductions on the respective typing derivations. Using the
\texttt{copy} constructor of OPEs is crucial to be able to go under
binders.
\end{proof}

Unlike the framing property, this theorem is not purely about the
type system: the term is indeed modified between the premise and
the conclusion. Now that we know that weakening is compatible with
the typing relations, let us study substitution.

%%%%%%%%%%%%%%%%%%
%% SUBSTITUTION %%
%%%%%%%%%%%%%%%%%%

\section{Substituting}\label{sec:substitution}

Stability of the typing relations under substitution guarantees
that the untyped evaluation of programs will yield results which have
the same type as well as preserve the linearity constraints.
The notion of leftovers naturally extends to substitutions: the
terms meant to be substituted for the variables in context which
are not used by a term will not be used when pushing the substitution
onto this term. They will therefore have to be returned as leftovers.

Because of this rather unusual behaviour for substitution, picking
the right type-theoretical representation for the environment
carrying the values to be substituted in is a bit subtle. Indeed,
relying on the usual combination of weakening and crafting a fresh
variable when going under a binder becomes problematic. The leftovers
returned by the induction hypothesis would then live in an extended
context and quite a lot of effort would be needed to downcast them
back to the smaller context they started in. The solution is to have
an explicit constructor for ``going under a binder'' which can be
simply peeled off on the way out of a binder. The values are still
weakened to fit in the extended context they end up in but that happens
at the point of use (i.e. when they are being looked up to replace a
variable) instead of when pushing the substitution under a binder.

\begin{definition}The environment \Env{} used to define substitution
for raw terms is indexed by two \Nat{}s $k$ and $l$ where $k$ is the
source's scope and $l$ is the target's scope. There are three constructors:
one for the empty environment ([]), one for going under a binder (\envextend{})
and one to extend an environment with an $\Inferable{l}$.
\begin{mathpar}
\type
 {k, l : \Nat{}
}{\Env{}(k, l) : \Set{}
}
\and \constructor
 {
}{[] : \Env{}(0, l)
}
\and \constructor
 {Ï : \Env{}(k, l)
}{Ï \envextend{} : \Env{}(\natsucc{k}, \natsucc{l})
}
\and \constructor
 {Ï : \Env{}(k, l) \and t : \Inferable{l}
}{Ï âˆ™ t : \Env{} (\natsucc{k}, l)
}
\end{mathpar}
\end{definition}

Environment are carrying \Inferable{} elements because, being in the
same syntactical class as \Var{}s, they can be substituted for them
without any issue. We now state the substitution lemma on untyped terms
because it is, unlike the one for weakening, non-standard by way of our
definition of environments.

\begin{lemma}Given a \Var{k} $v$ and an \Env{}(k, l) $\rho$, we can look up
the \Inferable{l} associated to $v$ in $\rho$.
\end{lemma}
\begin{proof} The proof goes by induction on $v$ and case analysis on $\rho$.
If the variable we look up has been introduced by a binder we went under using
the constructor \envextend{} then we return it immediately. Otherwise we get
our hands on a term which we may need to weaken. This corresponds to the following
functional specification (the practical implementation can be distinct to avoid
retraversing the term once for every single binder we went under):
\begin{figure}[ht]\centering
\begin{tabular}{l@{~[}l@{]\,=\,}l}
  \var{\varzero}      & \ensuremath{\rho \envextend{}} & \varzero{} \\
  \var{\varzero}      & \ensuremath{\rho âˆ™ t}          & \ensuremath{t} \\
  \var{(\varsucc{v})} & \ensuremath{\rho \envextend{}} & \texttt{lift}(\subst{\var{v}}{\rho}) \\
  \var{(\varsucc{v})} & \ensuremath{\rho âˆ™ t}          & \subst{\var{v}}{\rho} \\
\end{tabular}
\end{figure}

where \texttt{lift} is an instance of weakening defined in the previous
section which takes a term in a scope of size $k$ and returns the same
term in scope \natsucc{k}.
\end{proof}

\begin{lemma}Raw terms are stable under substitutions: for all $k$ and
$l$, given $t$ a term $\Inferable{k}$ (resp. $\Checkable{k}$) and $Ï$
an environment $\Env{}(k,l)$, we can apply the substitution $Ï$ to $t$
and obtain an $\Inferable{l}$ (resp. $\Checkable{l}$).
\end{lemma}
\begin{proof}By mutual induction on the raw terms. The traversals are purely
structural except when going under binders where the constructor \envextend{} is
used to extend the \Env{} appropriately. The prototypical case of a binder is
the \lam{} one, and its functional specification is: \mbox{\subst{(\lam{t})}{\rho} = \lam{(\subst{t}{\rho \envextend{}})}}.
\end{proof}

\begin{definition}The \textbf{environments} used when proving that Typing
Relations are stable under substitution follow closely the ones
for raw terms. \TEnv{Î˜â‚}{Î“}{Ï}{Î˜â‚‚} is a typing relation with
input usages $Î˜â‚$ and output $Î˜â‚‚$ for the raw substitution $Ï$
targeting the fresh variables in $Î“$. The typing for the empty environment has the same input and output
usages annotation. Formally:
\begin{mathpar}
\type
{{\begin{array}{l}
   Î¸ : \Context{l} \\
   Î˜â‚ : \Usages{Î¸} \\
   Î³ : \Context{k} \\
   Î“ : \Usages{Î³} \\ \\
  \end{array}}
 \and Ï : \Env{}(k,l)
 \and Î˜â‚‚ : \Usages{Î¸}
}{\TEnv{Î˜â‚}{Î“}{Ï}{Î˜â‚‚} : \Set{}
}
\and \constructor
 {
}{\TEnv{Î˜â‚}{[]}{[]}{Î˜â‚}
}
\end{mathpar}
For \textbf{f}resh variables in Î“, there are two cases depending on whether
they have been introduced by going under a binder or not. If it is
not the case then the typing environment carries around a typing
derivation for the term $t$ meant to be substituted for this variable.
Otherwise, it does not carry anything extra but tracks in its input /
output usages annotation the fact that the variable has been consumed.
\begin{mathpar}
\constructor
 {Î˜â‚ âŠ¢ t âˆˆ Ïƒ \andalso{} Î˜â‚‚ \and \TEnv{Î˜â‚‚}{Î“}{Ï}{Î˜â‚ƒ}
}{\TEnv{Î˜â‚}{Î“ âˆ™ \fresh{Ïƒ}}{Ï âˆ™ t}{Î˜â‚ƒ}
}
\and \constructor
 {\TEnv{Î˜â‚}{Î“}{Ï}{Î˜â‚‚}
}{\TEnv{Î˜â‚ âˆ™ \fresh{Ïƒ}}{Î“ âˆ™ \fresh{Ïƒ}}{Ï \envextend{}}{Î˜â‚‚ âˆ™ \stale{Ïƒ}}
}
\end{mathpar}
For \textbf{s}tale variables, there are two cases too. They are however
a bit more similar: none of them carry around an extra typing derivation.
The main difference is in the shape of the input and output context: in
the case for the ``going under a binder'' constructor, they are clearly
enriched with an extra (now consumed) variable whereas it is not the case
for the normal environment extension.
\begin{mathpar}
\constructor
 {\TEnv{Î˜â‚}{Î“}{Ï}{Î˜â‚‚}
}{\TEnv{Î˜â‚}{Î“âˆ™ \stale{Ïƒ}}{Ï âˆ™ t}{Î˜â‚‚}
}
\and \constructor
 {\TEnv{Î˜â‚}{Î“}{Ï}{Î˜â‚‚}
}{\TEnv{Î˜â‚ âˆ™ \stale{Ïƒ}}{Î“ âˆ™ \stale{Ïƒ}}{Ï \envextend{}}{Î˜â‚‚ âˆ™ \stale{Ïƒ}}
}
\end{mathpar}
\end{definition}

\begin{definition}
A Typing Relation \TR{\cdot} for a \Nat{}-indexed family $T$ equipped with
a function $\texttt{subst}_T$ which for all \Nat{}s $k, l$, given an
element $T_k$ and an $\Env{}(k,l)$ returns an element $T_l$ is said to
be \textbf{stable under substitution} if for all \Nat{}s $k$ and $l$, Î³ a $\Context{k}$,
Î“ and Î” two $\Usages{Î³}$, $t$ an element of $T_k$, Ïƒ a Type, Ï an $\Env{}(k,l)$,
$Î¸$ a $\Context{l}$ and $Î˜â‚$ and $Î˜â‚ƒ$ two $\Usages{Î¸}$ such that
\TR{k}$(Î“, t, Ïƒ, Î”)$ and \TEnv{Î˜â‚}{Î“}{Ï}{Î˜â‚ƒ} holds then there exists a $Î˜â‚‚$
of type $\Usages{Î¸}$ such that \TR{l}$(Î˜â‚, \texttt{subst}_T(t, Ï), Ïƒ, Î˜â‚‚)$ and
\TEnv{Î˜â‚‚}{Î”}{Ï}{Î˜â‚ƒ}.
\end{definition}

\begin{theorem}\label{theorem:substituting}
The Typing Relations for \Inferable{} and \Checkable{} are stable under substitution.
\end{theorem}
\begin{proof}
The proof by mutual structural induction on the typing derivations relies
heavily on the fact that these Typing Relations enjoy the framing property
in order to adjust the \Usages{} annotations.
\end{proof}


%%%%%%%%%%%%%%%%%%
%% FUNCTONALITY %%
%%%%%%%%%%%%%%%%%%

\section{Functionality}\label{sec:functional}

In the next section we will prove that type-checking and type-inference are
decidable. In the cases where the check fails, we have to prove that any
purported typing derivation would leave to a contradiction. Thse arguments
all follow a similar pattern: assuming that a typing derivation exist, we
use inversion lemmas to obtain results in direct contradiction to the
observations we have made. These inversion lemmas often rely on the fact
that the typing relations are functional.

Although we did highlight that some of our relations' indices are meant
to be seen as inputs whilst others are supposed to be outputs, we have
not yet made this relationship formal because this fact was seldom used
in the proofs so far. Functionality can be expressed by saying that given
a typing relation, if two typing derivations exist for some fixed arguments
(seen as inputs) then the other arguments (seen as outputs) are equal to each
other.

\begin{definition}We say that a relation $R$ of type
$Î (\mathit{ri} : \mathit{RI}). \mathit{II} â†’ O(ri) â†’ \Set{}$
is \textbf{functional} if for all \emph{r}elevant \emph{i}nputs $\mathit{ri}$, all pairs of
\emph{i}rrelevant \emph{i}nputs $\mathit{iiâ‚}$ and $\mathit{iiâ‚‚}$ and for all pairs
of \emph{o}utputs $oâ‚$ and $oâ‚‚$, if both $R(\mathit{ri}, \mathit{iiâ‚}, oâ‚)$
and $R(\mathit{ri}, \mathit{iiâ‚‚}, oâ‚‚)$ hold then $oâ‚ â‰¡ oâ‚‚$.
\end{definition}

\begin{lemma}The Typing Relations for \Var{} and \Inferable{} are functional
when seen as relations with relevant inputs the context and the scrutinee
(either a \Var{} or an \Inferable{}), irrelevant inputs their $\Usages{}$
annotation and outputs the inferred \Type{}s.
\end{lemma}

\begin{lemma}The Typing Relations for \Var{}, \Inferable{}, \Checkable{}
and \Env{} are functional when seen as relations with relevant inputs all
of their arguments except for one of the $\Usages{}$ annotation or the other.
This means that given a $\Usages{}$ annotation (whether the input one or the
output one) and the rest of the arguments, the other $\Usages{}$ annotation
is uniquely determined.
\end{lemma}

%%%%%%%%%%%%%%%%%%
%% TYPECHECKING %%
%%%%%%%%%%%%%%%%%%

\section{Typechecking}\label{sec:typechecking}

\begin{theorem}[Decidability of Typechecking]
\label{theorem:typechecking}
\textbf{Type-inference} for \Inferable{} and \textbf{Type-checking} for \Checkable{} are
decidable. In other words, given a \Nat{} $k$, Î³ a \Context{k} and
Î“ a $\Usages{Î³}$,
\begin{enumerate}
  \item for all \Inferable{k} $t$, we can decide if there is
        a \Type{} $Ïƒ$ and Î” a \Usages{Î³} such that $Î“ âŠ¢ t âˆˆ Ïƒ \andalso{} Î”$
  \item for all \Type{} Ïƒ and \Checkable{k} t, we can decide if
        there is Î” a \Usages{Î³} such that $Î“ âŠ¢ Ïƒ âˆ‹ t \andalso{} Î”$.
\end{enumerate}
\end{theorem}
\begin{proof}
The proof proceeds by mutual induction on the raw terms, using inversion
lemmas to dismiss the impossible cases, using auxiliary lemmas showing
that typechecking of \Var{}s and \Pattern{}s also is decidable and relies
heavily on the functionality of the various relations involved.
\end{proof}

One of the benefits of having a formal proof of a theorem in Agda is that
the theorem actually has computational content and may be run: the proof
is a decision procedure.

\begin{example}We can for instance check that the search procedure
succeeds in finding the \texttt{swapTyped} derivation we had written down
as Example~\ref{example:swapTyped}. Because Ïƒ and Ï„ are abstract in the
following snippet, the equality test checking that Ïƒ is equal to itself
and so is Ï„ does not reduce and we need to rewrite by the proof
\texttt{eq-diag} that the equality test always succeeds in this kind of
situation:
\begin{lstlisting}
  swapChecked : 'âˆ€' 'Ïƒ' 'Ï„' 'â†’' check [] (('Ïƒ âŠ— Ï„') 'âŠ¸' ('Ï„ âŠ— Ïƒ')) swap
                      'â‰¡' yes ([] , swapTyped)
  swapChecked 'Ïƒ' 'Ï„' rewrite eq-diag 'Ï„' | eq-diag 'Ïƒ' = refl
\end{lstlisting}
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%
%% EQUIVALENCE TO ILL %%
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Equivalence to ILL}\label{sec:equivalence}

We have now demonstrated that the $\Usages{}$-based formulation of
linear logic as a type system is amenable to mechanisation without
putting an unreasonable burden on the user. Indeed, the system's
important properties can all be dealt with by structural induction
and the user still retains the ability to write simple $Î»$-terms
which are not cluttered with structural rules.

However this presentation departs quite a lot from more traditional
formulations of intuitionistic linear logic. This naturally raises
the question of its correctness. In this section we recall a typical
presentation of Intuitionistic Linear Logic using a Sequent Calculus,
representing the multiset of assumptions as a list.

\subsection{A Sequent Calculus for Intuitionistic Linear Logic}

The definition of this calculus is directly taken from the Linear Logic
Wiki~\cite{wiki:linearlogic} whose notations we follow to the letter.
The interested reader will find more details in for instance Troelstra's
lectures~\cite{troelstra1991lectures}. In the following figure, Î³, Î´,
and Î¸ are context variables while Ïƒ, Ï„ , and Î½ range over types. We
overload the comma to mean both consing a single type to the front
of a list and appending two lists, as is customary.

\input{ill}

Our only departure from the traditional presentation is the \textit{mix}
rule which is an artefact of our encoding multisets as lists. It allows
the user to pick any interleaving Î¸ of two lists Î³ and Î´. This notion of
interleaving is formalised by the following three-place relation.

\begin{definition} The \textbf{interleaving} relation is defined by three
constructors: \textbf{[]} declares that interleaving two empty lists yields
the empty-list whilst \textbf{$\cdot,_{l}\cdot$} (and \textbf{$\cdot,_{r}\cdot$}
respectively) picks the head of the list on the left (the right respectively)
as the head of the interleaving and the tail as the result of interleaving
the rest.

\begin{mathpar}
\type{Î³, Î´, Î¸ : \List{a}}{Î³, Î´ â‰… Î¸ : \Set{}}
\and \constructor{ }{[] : [], [] â‰… []}
\and \constructor{p : Î³, Î´ â‰… Î¸}{Ïƒ ,_{l} p : (Ïƒ, Î³), Î´ â‰… (Ïƒ, Î¸)}
\and \constructor{p : Î³, Î´ â‰… Î¸}{Ïƒ ,_{r} p : Î³, (Ïƒ, Î´) â‰… (Ïƒ, Î¸)}
\end{mathpar}
\end{definition}

Now that we have our definition of the usual representation of Intuitionistic
Linear Logic (ILL), we are left with proving that the linear typing relation
we have defined is both sound and complete with respect to that logic.

\subsection{Soundness}

We start with the easiest part of the proof: soundness. This means that from
a typing derivation, we can derive a proof in ILL of what is essentially the
same statement. That is to say that if a term is said to have type Ïƒ in a fully
fresh context Î³ and proceeds to consume all of the resources in that context
during the typing derivation then it corresponds to a proof of Î³ âŠ¢ Ïƒ in ILL.

This statement needs to be generalised to be proven. Indeed, even if we start
with a context full of available resources, at the first split we encounter
(e.g. a tensor introduction or a function application), it won't be the case
anymore in one of the sub-terms. To state this more general formulation,
we need to introduce a new notion: used assumptions.

\begin{definition}The list of \textbf{used} assumptions in a proof Î“ âŠ† Î” in
the consumption partial order is the list of types which have turned from
fresh in Î“ to stale in Î”. The \used{\cdot} function is defined by recursion
over the proof that Î“ âŠ† Î”.
\end{definition}

\begin{definition}A Typing Relation \TR{} for terms $T$ is said to be \textbf{sound}
with respect to ILL if, for $k$ a \Nat{}, Î³ a \Context{k}, Î“ and Î” two \Usages{Î³},
t a term $T_k$ and Ïƒ a type, from the typing derivation \TR{}(Î“, t, Ïƒ, Î”) and p a
proof that Î“ âŠ† Î” we can derive $\used{p} âŠ¢ Ïƒ$.
\end{definition}

\begin{remark}The consumption lemma~\ref{lemma:consumption} guarantees that such
a proof Î“ âŠ† Î” always exists whenever the typing relation is either the one for
\Var{}, \Inferable{} or \Checkable{}.
\end{remark}

Before we can prove the soudness theorem, we need two auxiliary lemmas allowing
us to handle the mismatch we may have between the way the used assumptions of a
derivation are computed and the way the ones for its subderivations are obtained.

\begin{lemma}Given k a \Nat{}, Î³ a \Context{k} and Î“, Î”, and Î¸ three \Usages{Î³},
we have:\begin{enumerate}
\item if p and q are proofs that $Î“ âŠ† Î”$ then \used{p} = \used{q}.
\item if p is a proof that $Î“ âŠ† Î”$, q that $Î” âŠ† Î¸$ and pq that $Î“ âŠ† Î¸$ then \used{pq}
is an interleaving of \used{p} and \used{q}.
\end{enumerate}
\end{lemma}

The relation validating patterns is not a typing relation and as such it needs
to be handled separately. This can be done by defining a procedure elaborating
patterns away by showing that whenever $Ïƒ âˆ‹ p \leadsto{} Î³$, it is morally
acceptable to replace Ïƒ on the left by Î³. Which gives us the following \textit{cut}-like
admissible rule:

\begin{lemma}[Elaboration of Let-bindings] Provided k a \Nat{}, p a \Pattern{k},
Ïƒ a \Type{} and Î³ a \Context{k} such that $Ïƒ âˆ‹ p \leadsto{} Î³$, we have that for
all Î´ and Î¸ two \List{\Type} and Ï„ a \Type{}, if $Î´ âŠ¢ Ïƒ$ and $Î³ , Î¸ âŠ¢ Ï„$ then $Î´ , Î¸ âŠ¢ Ï„$.
\end{lemma}

We now have all the pieces to prove the soundness of our typing relations.

\begin{theorem}[Soundness]The typing relations for \Var{}, \Inferable{}
and \Checkable{} are all sound.
\end{theorem}
\begin{proof}
The proof is by mutual induction on the typing derivations. ILL's right
rules are in direct correspondence with our introduction rules. The
eliminators in our languages are translated by using ILL's \textit{cut}
together with left rules. The \textit{mix} rule is crucial to rewrite
the derivations' contexts on the fly.
\end{proof}

\subsection{Completeness}

Completeness is a trickier thing to prove: given a derivation in
the traditional sequent calculus, we need to build a corresponding
term and its typing derivation. However, unlike the soundness one
it does not give us any insight as to what the meaning of \Usages{}
and typing derivations is. So we only state the result and give
an idea of the proof.

\begin{theorem}[Completeness] Given Î³ a \List{\Type{}} and Ïƒ a type,
from a proof $Î³ âŠ¢ Ïƒ$ we can derive an \Inferable{} t and a proof that
$\fresh{Î³} âŠ¢ t âˆˆ Ïƒ \andalso{} \stale{Î³}$.
\end{theorem}
\begin{proof}
The proof is by induction over the derivation in ILL. The right rules
match our introduction rules well enough that they do not pose any
issue. Weakening lets us extend the \Usages{} of the sequents obtained
by induction hypothesis in the case of multi-premises rules. Left rules
are systematically translated as \cut{}s.

Finally, the hardest rule to handle is the \texttt{mix} rule which
reorganises the context. It is handled by a technical lemma which we
have left out of this paper. Informally: it states that if the input
and output \Usages{} of a typing derivation are obtained by the same
interleaving of two distinct pairs of \Usages{}, then for any other
interleaving we can find a term and a typing derivation for that term.
\end{proof}


%%%%%%%%%%%%%%%%%%
%% RELATED WORK %%
%%%%%%%%%%%%%%%%%%

\section{Related Work}

Benton, Bierman, de Paiva, and Hyland~\cite{benton1993term} did devise a
term assignment system for Intuitionistic Linear Logic which was stable
under substitution. Their system focuses on multiplicative linear logic
only when ours also encompasses additive connectives \emph{but} it gives
a thorough treatment of the \textit{!} modality. This is still an open
problem for us because we do not want to have the explicit handling of
\textit{!}'s weakening, contraction, dereliction, and promotion rules
pollute the raw terms.

Rand, Paykin and Zdancewic's work on modelling quantum circuits in
Coq~\cite{rand17qwire} necessarily includes a treatment of linearity
as qbits cannot be duplicated. And because it is mechanised, they have
to deal with the representation of contexts. Their focus is mostly on
the quantum aspect and they are happy relying on Coq's scripting
capabilities to cope with the extensional presentation.

Bob Atkey and James Wood~\cite{bob:sortingtypes} have been experimenting
with using a deep embedding of a linear lambda calculus in Agda as a way
to certify common algorithms. Being able to encode insertion sort as a
term in this deep embedding is indeed sufficient to conclude that the
output of the algorithm is a permutation of its input. They use on-the-fly
re-ordering of contexts via explicit permutation proofs.

Polakow faced with the task of embedding a linear Î»-calculus in
Haskell~\cite{polakow2016embedding} used a typed-tagless
approach~\cite{kiselyov2012typed} and tried to get as much automation
from typeclass resolution as possible. Seeing Haskell's typeclass
resolution mechanism as a Prolog-style proof search engine, he opted
for a relational description and thus an input-output presentation.
This system can handle multiplicatives, additives and is even extended
to a Dual Intuitionistic Linear Logic~\cite{barber1996dual} to accomodate
for values which can be duplicated. Focusing on the applications, it is
not proven to be stable under substitution or that the typechecking process
will always succeed.

The proof search community has been confronted with the inefficiency
of randomly splitting up the multiset of assumption when applying a
tensor-introduction rule. In an effort to combat this non-determinism,
they have introduced alternative sequent calculi returning
leftovers~\cite{cervesato1996efficient, winiko1994deterministic}.
However because they do not have to type a term living in a given
context, they do not care about the structure of the context of
assumptions: it is still modelled as a multiset.

We have already mentioned McBride's work~\cite{mcbride2016got}
on (as a first approximation: the setup is actually more general)
a type theory with a \emph{dependent linear} function space as a
very important source of inspiration. In that context it is indeed
crucial to retain the ability to talk about a resource even if it
has already been consumed. E.g. a function taking a boolean and
deciding whether it is equal to \texttt{tt} or \texttt{ff} will
have a type mentioning the function's argument twice. But in a
lawful manner: $(x : \DefinedType{Bool}) âŠ¸ (x â‰¡ \texttt{tt}) âˆ¨ (x â‰¡ \texttt{ff})$.
This leads to the need for a context \emph{shared} across all
subterms and consumption annotations ensuring that the linear
resources are never used more than once.

Finally, we can find a very concrete motivation for a predicate similar
to our $\Usage{}$ in Robbert Krebbers' thesis~\cite{krebbers2015thesis}.
In section 2.5.9, he describes one source of undefined behaviours
in the C standard: the execution order of expressions is unspecified
thus leaving the implementers with absolute freedom to pick any order
they like if that yields better performances. To make their life
simpler, the standard specifies that no object should be modified
more than once during the execution of an expression. In order to
enforce this invariant, Krebbers' memory model is enriched with extra
information:
\begin{quote}
  [E]ach bit in memory carries a permission that is set to a special
  locked permission when a store has been performed. The memory
  model prohibits any access (read or store) to objects with locked
  permissions. At the next sequence point, the permissions of locked
  objects are changed back into their original permission, making
  future accesses possible again.
\end{quote}

\section{Conclusion}

We have shown that taking seriously the view of linear logic as a
logic of resource consumption leads, in type theory, to a well-behaved
presentation of the corresponding type system for the lambda-calculus.
The framing property claims that the state of irrelevant resources does
not matter, stability under weakening shows that one may even add extra
irrelevant assumptions to the context and they will be ignored whilst
stability under substitution guarantees subject reduction with respect
to the usual small step semantics of the lambda calculus. Finally, the
decidability of type checking makes it possible to envision a user-facing
language based on raw terms and top-level type annotations where the
machine does the heavy lifting of checking that all the invariants are
met whilst producing a certified-correct witness of typability.

Avenues for future work include a treatment of an \emph{affine} logic
where the type of substitution will have to be be different because
of the ability to throw away resources without using them. Our long
term goal is to have a formal specification of a calculus for Probabilistic
and Bayesian Reasoning similar to the affine one described by Adams
and Jacobs~\cite{Adams2015Type}.
Another interesting question is whether these resource annotations
can be used to develop a fully formalised proof search procedure for
intuitionistic linear logic. The author and McBride have made an effort
in such a direction~\cite{Allais2015Proof} by designing a sound and
complete search procedure for a fragment of intuitionistic linear logic
with type constructors tensor and with. Its extension to lolipop is
currently an open question.

%%
%% Bibliography
%%

%% Either use bibtex (recommended),

\bibliography{main}

%% .. or use the thebibliography environment explicitely

\newpage
\appendix

\section{Fully-expanded Typing Derivation for \texttt{swap}}
\input{typing-swap}

\end{document}
